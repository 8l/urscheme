I don’t yet have a separate profiler for Ur-Scheme; however,
Cachegrind now sort of works.

Cachegrind
==========

If you aren’t familiar with Cachegrind
--------------------------------------

Run your program under cachegrind; if your normal command-line is
`myprogram < baz > quux`, then you can run `valgrind --tool=cachegrind
myprogram < baz > quux`.  This produces some general reporting
information on stderr and more detailed information in a file called
cachegrind.out.*pid*, which you can inspect with e.g. `kcachegrind` by
running `kcachegrind cachegrind.out.`*pid*.

Ur-Scheme and Cachegrind
------------------------

Ur-Scheme adds a minimal amount of symbol information to its output
files so that Cachegrind can produce slightly useful per-function
output.  The labels in the assembly output file will usually, but not
always, have some vague connection to the function names in the Scheme
source, adjusted to be valid assembly-language symbol names.

Typically they will end in `_3` when they are what you’d expect;
function names ending in some other number might be anonymous
functions or functions whose desired symbol names conflicted with
something already defined.  For example, if you redefine something in
the standard library, or if you define something twice, or if you
define functions named both `foo?` and `fooP`, then you will get a
function that’s just named after whatever came immediately before it.

I haven’t tried adding line-number information yet for three reasons:

* I don’t know anything about debugging information 
* Ur-Scheme doesn’t track line-number information from the source file
* R5RS Scheme programs don’t have a way to take command-line
  arguments, so Ur-Scheme takes input on stdin, so it doesn’t know the
  name of the input file.

A lot of the run time (25% when profiling compiling the compiler with
itself) is still in “(unknown)”, so I still have some work to do.

Notes about how to write a profiler
===================================

RDPMC and RDTSC
---------------

The x86 has RDPMC and RDTSC instructions, which could be useful for
profiling.

RDPMC reads the performance-monitoring counter specified by ECX into
EDX:EAX.  “P6” processors like my CPU have both a “Valid PMC Index
Range” and a “General-purpose Counters” of “0, 1”.

> “The performance-monitoring counters are event counters that can
> be programmed to count events such as the number of instructions
> decoded, number of interrupts received, or number of cache
> loads. Appendix A, “Performance Monitoring Events,” in the Intel®
> 64 and IA-32 Architectures Software Developer’s Manual, Volume 3B,
> lists the events that can be counted for various processors in the
> Intel 64 and IA-32 architecture families.”

That manual (Intel document 253669) also contains a “performance
monitoring overview” section.

RDTSC reads the current value of the processor’s time-stamp counter (a
64-bit MSR) into EDX:EAX.  It counts the number of CPU cycles.  Linux
doesn’t seem to save and restore this (64-bit) counter on process
switch.  (I don’t even know if that’s possible.)  

Sources of Measurement Error with RDTSC
---------------------------------------

A simple printf loop in C with a call to a tiny assembly routine
(“rdtsc: rdtsc; ret”) gets timings of 521-567 clock cycles >99% of the
time, but then:

    >1000 cycles: 256 times out of 100 000
    >2000 cycles: 146 times out of 100 000
    >5000 cycles: 137 times out of 100 000
    >10000 ccles: 132 times out of 100 000
    >20000 ccles: 123 times out of 100 000
    >50000 cycles: 38 times out of 100 000
    >100k cycles:  24 times out of 100 000
    >200k cycles:  22 times out of 100 000
    >500k cycles:  11 times out of 100 000
    >1M   cycles:   1 time out of 100 000 (1 187 426 clock cyles)

The total of 68 532 669 cycles was somewhere around 0.1 seconds.  So
it seems that somewhere around 1000-2500 times a second, there’s some
kind of interruption that could cause a measurement error; that the
measurement errors are mostly on the order of 1000 cycles or on the
order of 20000-50000 cycles, but of course they can be arbitrarily
large; that about 18% of this little program’s run-time was in these
interruptions, mostly in the top 12 (8.5 million cycles out of 68
million).  So I’m not quite sure how to get useful profiling results
out of this.

I wrote an assembly program that just calls RDTSC in a tight loop, ten
million times.  This takes 0.688 seconds of user CPU time, so it is
taking 70ns to do each iteration of the loop.  With two RDTSC
instructions in the loop, it took 1.460 seconds, and with three it
took 2.184 seconds, so >95% the 70ns is attributable to RDTSC and not
the loop.  So RDTSC on my machine is probably only useful for
measuring things that take a long time compared to 70ns, say 700ns or
a microsecond or more.  Unfortunately, on the same CPU, Ur-Scheme
function call and return takes only about 250ns or so, so if I put a
RDTSC on every function entry and exit, a large part of the time it
would measure would be the run-time of RDTSC instructions in other
routines.

There are rumors that the CPUs of SMP machines, which includes
essentially all new PCs, have desynchronized TSC counters.  That would
make RDTSC less useful.  Douglas Schmidt’s ACE’s `High_Res_Timer.h`
says:

     * Another problem arises with multiprocessor computers, there
     * are reports that the different RDTSC’s are not always kept
     * in sync.

Etc
---

I haven’t penetrated enough into the “Intel® 64 and IA-32
Architectures Software Developer’s Manual, Volume 3B” to be able to
figure out if RDPMC is potentially useful on my machine.
